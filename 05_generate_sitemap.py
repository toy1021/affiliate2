#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json
import os
from datetime import datetime, timedelta, timezone
from xml.etree.ElementTree import Element, SubElement, tostring
from xml.dom import minidom
import hashlib

def generate_main_sitemap():
    """ãƒ¡ã‚¤ãƒ³ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚’ç”Ÿæˆ"""
    
    # ã‚µã‚¤ãƒˆæƒ…å ±ã‚’èª­ã¿è¾¼ã¿
    if os.path.exists('output/sitemap.json'):
        with open('output/sitemap.json', 'r', encoding='utf-8') as f:
            site_data = json.load(f)
    else:
        print("Error: sitemap.json not found")
        return False
    
    # XMLè¦ç´ ã‚’ä½œæˆ
    urlset = Element('urlset')
    urlset.set('xmlns', 'http://www.sitemaps.org/schemas/sitemap/0.9')
    urlset.set('xmlns:image', 'http://www.google.com/schemas/sitemap-image/1.1')
    
    base_url = 'https://toy1021.github.io/affiliate2/'
    jst = timezone(timedelta(hours=9))
    last_mod = datetime.now(jst).strftime('%Y-%m-%dT%H:%M:%S+09:00')
    
    # ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸
    url = SubElement(urlset, 'url')
    SubElement(url, 'loc').text = base_url
    SubElement(url, 'lastmod').text = last_mod
    SubElement(url, 'changefreq').text = 'hourly'
    SubElement(url, 'priority').text = '1.0'
    
    # ã‚«ãƒ†ã‚´ãƒªãƒšãƒ¼ã‚¸ï¼ˆä»®æƒ³URLï¼‰
    categories = ["AIãƒ»æ©Ÿæ¢°å­¦ç¿’", "Appleè£½å“", "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°", "ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼", "Googleãƒ»Android"]
    for category in categories:
        url = SubElement(urlset, 'url')
        category_slug = hashlib.md5(category.encode('utf-8')).hexdigest()[:8]
        SubElement(url, 'loc').text = f"{base_url}#category-{category_slug}"
        SubElement(url, 'lastmod').text = last_mod
        SubElement(url, 'changefreq').text = 'daily'
        SubElement(url, 'priority').text = '0.9'
    
    # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³
    for page_num in range(1, site_data['total_pages'] + 1):
        url = SubElement(urlset, 'url')
        if page_num == 1:
            SubElement(url, 'loc').text = base_url
        else:
            SubElement(url, 'loc').text = f"{base_url}#page-{page_num}"
        SubElement(url, 'lastmod').text = last_mod
        SubElement(url, 'changefreq').text = 'daily'
        SubElement(url, 'priority').text = '0.8' if page_num <= 3 else '0.6'
    
    # XMLã‚’æ•´å½¢ã—ã¦ä¿å­˜
    save_xml(urlset, 'output/sitemap.xml', 'sitemap.xml')
    
    print(f"âœ… Main Sitemap generated:")
    print(f"  - Total pages: {site_data['total_pages']}")
    print(f"  - Categories: {len(categories)}")
    
    return True

def generate_news_sitemap():
    """Google Newsç”¨ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚’ç”Ÿæˆ"""
    
    # è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    if not os.path.exists('output/articles.json'):
        print("Error: articles.json not found")
        return False
        
    with open('output/articles.json', 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # æœ€æ–°48æ™‚é–“ä»¥å†…ã®è¨˜äº‹ã®ã¿ã‚’å«ã‚ã‚‹ï¼ˆGoogle Newsã®è¦ä»¶ï¼‰
    jst = timezone(timedelta(hours=9))
    cutoff_time = datetime.now(jst) - timedelta(hours=48)
    
    urlset = Element('urlset')
    urlset.set('xmlns', 'http://www.sitemaps.org/schemas/sitemap/0.9')
    urlset.set('xmlns:news', 'http://www.google.com/schemas/sitemap-news/0.9')
    
    articles_added = 0
    
    # å…¨è¨˜äº‹ã‚’ãƒã‚§ãƒƒã‚¯
    for page in data.get('pages', []):
        for article in page.get('articles', []):
            # è¨˜äº‹ã®å…¬é–‹æ—¥ã‚’ãƒã‚§ãƒƒã‚¯
            try:
                pub_date_str = article.get('pubDate', '')
                if pub_date_str:
                    # è¨˜äº‹ã®å…¬é–‹æ—¥ã‚’ãƒ‘ãƒ¼ã‚¹ï¼ˆç°¡å˜ãªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’æƒ³å®šï¼‰
                    article_date = datetime.strptime(pub_date_str[:19], '%Y-%m-%d %H:%M:%S')
                    article_date = article_date.replace(tzinfo=jst)
                    
                    # 48æ™‚é–“ä»¥å†…ã®è¨˜äº‹ã®ã¿è¿½åŠ 
                    if article_date > cutoff_time and articles_added < 100:  # æœ€å¤§100è¨˜äº‹
                        url = SubElement(urlset, 'url')
                        SubElement(url, 'loc').text = article.get('link', '')
                        SubElement(url, 'lastmod').text = article_date.strftime('%Y-%m-%dT%H:%M:%S+09:00')
                        
                        # Google Newsè¦ç´ 
                        news = SubElement(url, 'news:news')
                        publication = SubElement(news, 'news:publication')
                        SubElement(publication, 'news:name').text = 'æ—¥æœ¬ã®ãƒ†ãƒƒã‚¯ãƒ‹ãƒ¥ãƒ¼ã‚¹é€Ÿå ±'
                        SubElement(publication, 'news:language').text = 'ja'
                        
                        SubElement(news, 'news:publication_date').text = article_date.strftime('%Y-%m-%dT%H:%M:%S+09:00')
                        SubElement(news, 'news:title').text = article.get('title', '')[:100]
                        
                        # ã‚¸ãƒ£ãƒ³ãƒ«/ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è¿½åŠ 
                        keywords = article.get('category', 'ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼')
                        SubElement(news, 'news:keywords').text = keywords
                        
                        articles_added += 1
            except:
                continue
    
    # XMLã‚’ä¿å­˜
    save_xml(urlset, 'output/news_sitemap.xml', 'news_sitemap.xml')
    
    print(f"âœ… News Sitemap generated:")
    print(f"  - Recent articles: {articles_added}")
    
    return True

def generate_sitemap_index():
    """ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ç”Ÿæˆ"""
    
    sitemapindex = Element('sitemapindex')
    sitemapindex.set('xmlns', 'http://www.sitemaps.org/schemas/sitemap/0.9')
    
    base_url = 'https://toy1021.github.io/affiliate2/'
    jst = timezone(timedelta(hours=9))
    last_mod = datetime.now(jst).strftime('%Y-%m-%dT%H:%M:%S+09:00')
    
    # ãƒ¡ã‚¤ãƒ³ã‚µã‚¤ãƒˆãƒãƒƒãƒ—
    sitemap = SubElement(sitemapindex, 'sitemap')
    SubElement(sitemap, 'loc').text = f"{base_url}sitemap.xml"
    SubElement(sitemap, 'lastmod').text = last_mod
    
    # ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆãƒãƒƒãƒ—
    sitemap = SubElement(sitemapindex, 'sitemap')
    SubElement(sitemap, 'loc').text = f"{base_url}news_sitemap.xml"
    SubElement(sitemap, 'lastmod').text = last_mod
    
    # XMLã‚’ä¿å­˜
    save_xml(sitemapindex, 'output/sitemap_index.xml', 'sitemap_index.xml')
    
    print(f"âœ… Sitemap Index generated")
    
    return True

def save_xml(element, output_path, root_path):
    """XMLã‚’æ•´å½¢ã—ã¦ä¿å­˜"""
    rough_string = tostring(element, 'utf-8')
    reparsed = minidom.parseString(rough_string)
    pretty_xml = reparsed.toprettyxml(indent="  ")
    
    # XMLå®£è¨€ã‚’å«ã‚€å®Œå…¨ãªXMLã¨ã—ã¦ä¿å­˜
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(pretty_xml)
    
    # ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚‚ã‚³ãƒ”ãƒ¼
    with open(root_path, 'w', encoding='utf-8') as f:
        f.write(pretty_xml)

def generate_sitemap():
    """å…¨ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚’ç”Ÿæˆ"""
    print("=== Enhanced Sitemap Generator ===")
    
    success = True
    success &= generate_main_sitemap()
    success &= generate_news_sitemap()
    success &= generate_sitemap_index()
    
    if success:
        print("\nğŸ‰ All sitemaps generated successfully!")
        print("  - sitemap.xml (main)")
        print("  - news_sitemap.xml (Google News)")
        print("  - sitemap_index.xml (index)")
    
    return success

if __name__ == "__main__":
    print("=== XML Sitemap Generator ===")
    generate_sitemap()