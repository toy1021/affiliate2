#!/usr/bin/env python3

"""
XMLã‚µã‚¤ãƒˆãƒãƒƒãƒ—ç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
Googleæ¤œç´¢ã§ã®ç™ºè¦‹æ€§ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã®ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚’ç”Ÿæˆ
"""

import json
import os
from datetime import datetime, timezone
from urllib.parse import quote

def generate_xml_sitemap():
    """XMLã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚’ç”Ÿæˆ"""
    
    # ã‚µã‚¤ãƒˆåŸºæœ¬æƒ…å ±
    base_url = "https://toy1021.github.io/affiliate2"
    current_time = datetime.now(timezone.utc).strftime('%Y-%m-%dT%H:%M:%S+00:00')
    
    # è¨˜äº‹ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    articles_file = "docs/articles.json"
    if not os.path.exists(articles_file):
        print(f"âŒ è¨˜äº‹ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {articles_file}")
        return False
    
    with open(articles_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    articles = data.get("articles", [])
    
    # XMLã‚µã‚¤ãƒˆãƒãƒƒãƒ—ç”Ÿæˆ
    xml_content = ['<?xml version="1.0" encoding="UTF-8"?>']
    xml_content.append('<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">')
    
    # 1. ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ï¼ˆæœ€é«˜å„ªå…ˆåº¦ï¼‰
    xml_content.append('  <url>')
    xml_content.append(f'    <loc>{base_url}/</loc>')
    xml_content.append(f'    <lastmod>{current_time}</lastmod>')
    xml_content.append('    <changefreq>hourly</changefreq>')
    xml_content.append('    <priority>1.0</priority>')
    xml_content.append('  </url>')
    
    # 2. ã‚«ãƒ†ã‚´ãƒªãƒšãƒ¼ã‚¸ï¼ˆé«˜å„ªå…ˆåº¦ï¼‰
    categories = data.get("categories", [])
    for category in categories:
        cat_name = category.get("name", "")
        if cat_name:
            # URLã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
            encoded_cat = quote(cat_name)
            xml_content.append('  <url>')
            xml_content.append(f'    <loc>{base_url}/#category={encoded_cat}</loc>')
            xml_content.append(f'    <lastmod>{current_time}</lastmod>')
            xml_content.append('    <changefreq>daily</changefreq>')
            xml_content.append('    <priority>0.8</priority>')
            xml_content.append('  </url>')
    
    # 3. è¨˜äº‹ãƒšãƒ¼ã‚¸ï¼ˆä¸­å„ªå…ˆåº¦ï¼‰
    # æœ€æ–°50è¨˜äº‹ã®ã¿ã‚’å«ã‚ã‚‹ï¼ˆé‡è¦è¨˜äº‹ã®ã¿ï¼‰
    top_articles = articles[:50] if len(articles) > 50 else articles
    
    for article in top_articles:
        # è¨˜äº‹ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯IDç”Ÿæˆï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã®ãƒãƒƒã‚·ãƒ¥ï¼‰
        title = article.get("title", "")
        if title:
            import hashlib
            article_id = hashlib.md5(title.encode()).hexdigest()[:8]
            
            # è¨˜äº‹ã®å…¬é–‹æ—¥å–å¾—
            published = article.get("published_date", current_time)
            if published and published != "":
                # ISO format ã«å¤‰æ›
                try:
                    if "T" not in published:
                        # æ—¥ä»˜ã®ã¿ã®å ´åˆã¯æ™‚åˆ»ã‚’è¿½åŠ 
                        published += "T00:00:00+00:00"
                    elif "+" not in published and "Z" not in published:
                        # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³æƒ…å ±ãŒãªã„å ´åˆã¯è¿½åŠ 
                        published += "+00:00"
                except:
                    published = current_time
            else:
                published = current_time
            
            xml_content.append('  <url>')
            xml_content.append(f'    <loc>{base_url}/#article={article_id}</loc>')
            xml_content.append(f'    <lastmod>{published}</lastmod>')
            xml_content.append('    <changefreq>weekly</changefreq>')
            xml_content.append('    <priority>0.6</priority>')
            xml_content.append('  </url>')
    
    xml_content.append('</urlset>')
    
    # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
    sitemap_content = '\n'.join(xml_content)
    
    # è¤‡æ•°ã®å ´æ‰€ã«å‡ºåŠ›
    output_files = [
        "sitemap.xml",
        "docs/sitemap.xml", 
        "output/sitemap.xml"
    ]
    
    for output_file in output_files:
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else ".", exist_ok=True)
        
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(sitemap_content)
            print(f"âœ… ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚’ç”Ÿæˆã—ã¾ã—ãŸ: {output_file}")
        except Exception as e:
            print(f"âŒ ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {output_file} - {e}")
    
    # çµ±è¨ˆæƒ…å ±è¡¨ç¤º
    total_urls = 1 + len(categories) + len(top_articles)  # ãƒ¡ã‚¤ãƒ³ + ã‚«ãƒ†ã‚´ãƒª + è¨˜äº‹
    print(f"ğŸ“Š ã‚µã‚¤ãƒˆãƒãƒƒãƒ—çµ±è¨ˆ:")
    print(f"   - ç·URLæ•°: {total_urls}")
    print(f"   - ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸: 1")
    print(f"   - ã‚«ãƒ†ã‚´ãƒªãƒšãƒ¼ã‚¸: {len(categories)}")
    print(f"   - è¨˜äº‹ãƒšãƒ¼ã‚¸: {len(top_articles)}")
    
    return True

if __name__ == "__main__":
    print("ğŸ—ºï¸ XMLã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚’ç”Ÿæˆä¸­...")
    success = generate_xml_sitemap()
    
    if success:
        print("âœ… XMLã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã®ç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸ")
    else:
        print("âŒ XMLã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")